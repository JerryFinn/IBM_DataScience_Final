{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Capstone Project: Battle of the Neighborhoods\n",
    "## Chicago Neighborhood Opportunity Fund Allocation  \n",
    "\n",
    "See the main part of this project in [Capstone_BattleOfNeighborhoods.ipynb](Capstone_BattleOfNeighborhoods.html).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes about getting and saving census tract data\n",
    "\n",
    "1. My cloud environments have some resource limitations. My home PC is relatively well resourced\n",
    "2. Installing geopandas on my home PC aborted due to dependency issues \n",
    "3. I have geopandas in some cloud environments\n",
    "4. I'll be doing much of the processing on my home PC, but use geopandas in the cloud. Therefore some cells will be specific to certain environments, but I will identified where cells are dependent on specific environments  \n",
    "5. Create GeoJSON and CSV files for Cook County (Chicago) Census Tracts\n",
    "6. Read list of Invest SOUTH/WEST RFP locations, assign a Census Tract number, and write to a GeoJSON and CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input-Output\n",
    "\n",
    "**Expected Input**  \n",
    "The following file is created by the main report notebook [Capstone_BattleOfNeighborhoods.ipynb](Capstone_BattleOfNeighborhoods.html). It must exist to run this notebook  \n",
    "`./data/isw_rfp_updated.csv`  \n",
    "\n",
    "**Expected Output**  \n",
    "The following files will be needed by the main report notebook [Capstone_BattleOfNeighborhoods.ipynb](Capstone_BattleOfNeighborhoods.html) as input. They must exist in the local directory structure of the main report for it to finish.  \n",
    "`./data/cook_tract.geojson`  \n",
    "`./data/cook_tract.csv`   \n",
    "`./data/isw_rfp_tract.csv`  \n",
    "`./data/isw_rfp_tract.geojson`  \n",
    "\n",
    "\n",
    "## Introduction <a name=\"Introduction\"></a>\n",
    "\n",
    "While researching a topic for this capstone project I came across an academic paper that influenced where millions of dollars of development funds will be used. The \"Neighborhood Opportunity Fund\" [(NOF)](https://neighborhoodopportunityfund.com) in Chicago can award up to \\$2.5 million for individual projects in \"Qualified Investment Areas\". Information on the program explained that the geographic areas that were choosen to receive the grants was influenced by the study [\"Mapping the DNA of Urban Neighborhoods: Clustering Longitudinal Sequences of Neighborhood Socioeconomic Change\"](https://www.tandfonline.com/doi/full/10.1080/00045608.2015.1096188) by Elizabeth Delmelle. The study classified neighborhoods by socioeconomic, housing, and demographic variables using US census data from the years of 1970 to 2010, therefore it could be used to identify areas in need of investment. Throughout this report I will refer to the study as \"Dr. Delmelle's study\", the \"DNA of Urban Neighborhoods study\", or the \"2010 study\". 2010 is the year of the latest data used in the study, not its publication date.   \n",
    "\n",
    "This project will try to repeat and extend this study with the lastest census data (2019), then compare the new results to the locations of where some of the funds are going to be awarded. The objective will be to evaluate if the funds, (given the updated information), are going to neighborhoods most in need of investment. The original study examined both Chicago and Los Angeles. This project will only address Chicago.  \n",
    "  \n",
    "An organization called [INVEST South/West](https://www.chicago.gov/city/en/sites/invest_sw/home.html) is in charge of allocating much of the NOF for parts of Chicago. They have an [RFP page](https://www.chicago.gov/city/en/sites/invest_sw/home/requests-for-proposals.html) explaining how developers can solict these funds for projects in predetermined locations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up file names to work on Windows or Linux file system\n",
    "import os\n",
    "datadir = os.path.join('.', 'data')\n",
    "if not os.path.exists(datadir):\n",
    "    os.makedirs(datadir)\n",
    "censusdir = os.path.join(datadir, 'census')\n",
    "if not os.path.exists(censusdir):\n",
    "    os.makedirs(censusdir)\n",
    "\n",
    "zip_tigerline_ill_tract = os.path.join(censusdir, 'tl_2010_17031_tract10.zip')\n",
    "cook_tract_geojson_file = os.path.join(datadir, 'cook_tract.geojson') \n",
    "cook_tract_csv_file = os.path.join(datadir, 'cook_tract.csv')  \n",
    "\n",
    "isw_rfp_updated_csv=os.path.join(datadir, 'isw_rfp_updated.csv')\n",
    "isw_rfp_tract_csv=os.path.join(datadir, 'isw_rfp_tract.csv')\n",
    "isw_rfp_tract_geojson=os.path.join(datadir, 'isw_rfp_tract.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries used in all environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import json\n",
    "except ImportError as e:\n",
    "    !pip install json\n",
    "    import json\n",
    "\n",
    "try:\n",
    "    import shapefile\n",
    "except ImportError as e:\n",
    "    !pip install pyshp\n",
    "    import shapefile\n",
    "    \n",
    "try:\n",
    "    import zipfile\n",
    "except ImportError as e:\n",
    "    !pip install zipfile\n",
    "    import zipfile\n",
    "    \n",
    "try:\n",
    "    from io import StringIO\n",
    "    from io import BytesIO\n",
    "except ImportError as e:\n",
    "    !pip install io\n",
    "    from io import StringIO\n",
    "    from io import BytesIO\n",
    "    \n",
    "try:\n",
    "    import itertools\n",
    "    from itertools import zip_longest\n",
    "except ImportError as e:\n",
    "    !pip install itertools\n",
    "    import itertools\n",
    "    from itertools import zip_longest\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import platform\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell installs are for GeoPandas. I only run these in my IBM cloud environment since my home PC always errors out while trying to install one of the prerequisites for GeoPandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import rtree\n",
    "except ImportError as e:\n",
    "    !pip install rtree\n",
    "    import rtree\n",
    "\n",
    "try:\n",
    "    import pygeos\n",
    "except ImportError as e:\n",
    "    !pip install pygeos\n",
    "    import pygeos\n",
    "\n",
    "try:\n",
    "    import psycopg2\n",
    "except ImportError as e:\n",
    "    !pip install psycopg2\n",
    "    import psycopg2\n",
    "\n",
    "try:\n",
    "    import geopandas as gpd\n",
    "    from geopandas.tools import overlay\n",
    "except ImportError as e:\n",
    "    !pip install geopandas\n",
    "    import geopandas as gpd\n",
    "    from geopandas.tools import overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Census Tracts\n",
    "\n",
    "Get the TigerLine (Shape Files) from the Census Department for Cook County. The Census department does not have GeoJson, which which works well with GeoPandas, so we will convert them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup functions for the conversion to GeoJson\n",
    "\n",
    "def make_geojson(content, gfile, tocrs=\"\"):\n",
    "    # Convert a shape file to a GeoJSON\n",
    "    # Parameters:\n",
    "    #  content: A zip file containing shape files\n",
    "    #  gfile: A string with a file name to save the GeoJSON file\n",
    "    #  tocsr: A coordinate system to convert the coordinates to\n",
    "    # Returns: None, but does write a file to disk\n",
    "    shp_file=\"\"\n",
    "    with zipfile.ZipFile(content) as f:\n",
    "        for name in f.namelist():\n",
    "            if name.endswith('.shp'):\n",
    "                shp_file=os.path.join(censusdir, name)\n",
    "                f.extract(name, censusdir)\n",
    "            if name.endswith('.shx'):\n",
    "                f.extract(name, censusdir)\n",
    "            if name.endswith('.prj'):\n",
    "                f.extract(name, censusdir)\n",
    "            if name.endswith('.dbf'):\n",
    "                f.extract(name, censusdir)\n",
    "                \n",
    "    file = gpd.read_file(shp_file)\n",
    "    if tocrs != \"\":\n",
    "        crs_file=file.to_crs(crs=tocrs)\n",
    "        crs_file.to_file(gfile, driver='GeoJSON')\n",
    "    else:\n",
    "        file.to_file(gfile, driver='GeoJSON')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download shapefiles for Census Tract, which are sets of Census Block Groups, but smaller than a Public Use Microdata Area and Neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2047k    0 2047k    0     0  4519k      0 --:--:-- --:--:-- --:--:-- 4519k\n"
     ]
    }
   ],
   "source": [
    "if platform.system() == 'Windows':\n",
    "    #  Use this to dowload the Illinois Census Tract shape files only on Windows (home) PC\n",
    "    !curl https://www2.census.gov/geo/pvs/tiger2010st/17_Illinois/17031/tl_2010_17031_tract10.zip --output .\\data\\census\\tl_2010_17031_tract10.zip\n",
    "else:\n",
    "    #  Use this to dowload the CookCounty Census Tract shape files on Linux Environment\n",
    "    !curl https://www2.census.gov/geo/pvs/tiger2010st/17_Illinois/17031/tl_2010_17031_tract10.zip --output ./data/census/tl_2010_17031_tract10.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cells will assume we have GeoPandas installed in the current environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: Census tract CRS is NAD83\n",
    "make_geojson(zip_tigerline_ill_tract, cook_tract_geojson_file, tocrs=\"WGS84\")\n",
    "# Delete the fields we don't need and give more intuitive names to those we do \n",
    "cook_tract_geojson=gpd.read_file(cook_tract_geojson_file)\n",
    "cook_tract_geojson=cook_tract_geojson.rename(columns={\"GEOID10\": \"CensusTract\", 'COUNTYFP10': 'County'})\n",
    "cook_tract_geojson=cook_tract_geojson.drop(['STATEFP10', 'TRACTCE10', 'NAME10', 'NAMELSAD10', 'MTFCC10', \n",
    "                                                          'FUNCSTAT10', 'ALAND10', 'AWATER10'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the new files to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cook_tract_geojson.to_csv(cook_tract_csv_file, index=False)\n",
    "cook_tract_geojson.to_file(cook_tract_geojson_file, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only in the IBM cloud environment for the next cell. A below cell should be removed when uploading this notebook to github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "credentials = {\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell if we are in the IBM Cloud ONLY. \n",
    "# Assume that GeoJSON file with cook county only created in the previous cell has be uploaded to IBM Cloud Object Storage\n",
    "# We retrieve the object and write to a local file\n",
    "import types\n",
    "import ibm_boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "cos = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id=credentials['IBM_API_KEY_ID'],\n",
    "    ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url=credentials['ENDPOINT'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files Downloaded\n"
     ]
    }
   ],
   "source": [
    "key_isw_rfp_updated_csv=os.path.basename(isw_rfp_updated_csv)\n",
    "try:\n",
    "    res=cos.download_file(Bucket=credentials['BUCKET'],Key=key_isw_rfp_updated_csv,\n",
    "                          Filename=isw_rfp_updated_csv)\n",
    "except Exception as e:\n",
    "    print(Exception, e)\n",
    "else:\n",
    "    print('Files Downloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign the NOF RFP Location to Census Tract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell is where we read `./data/isw_rfp_updated.csv` and assign it a census tract number. Function that will assign the point (RFP location) or a smaller geographic polygon to a be within a larger polygon (census tract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://medium.com/analytics-vidhya/point-in-polygon-analysis-using-python-geopandas-27ea67888bff\n",
    "def get_pip (points, plane): \n",
    "    # Parameters:\n",
    "    #  points: geopanda dataframe with points or polygons to be assigned to larger polygons\n",
    "    #  plane: geopanda dataframe with larger planes(polygons) which contain areas in points \n",
    "    # Returns: new geopanda dataframe layed out like points with polygon from plane assigned\n",
    "    \n",
    "    # Validate we can handle the type of geometry\n",
    "    if points.geom_type[0] == 'Point':\n",
    "        print(\"\\nWorking with Point.\")\n",
    "    elif (points.geom_type[0] == 'Polygon' or points.geom_type[0] == 'MultiPolygon') :\n",
    "        print(\"\\nWorking with Polygon.\")\n",
    "    else:\n",
    "        print('Abort function. Cannot handle geom_type ', points.geom_type[0])\n",
    "        return\n",
    "    h_list = list(plane.larger_plane)\n",
    "    # Create empty dataframe\n",
    "    df = gpd.GeoDataFrame().reindex_like(points).dropna()\n",
    "    for h in h_list:\n",
    "        # Get geometry for specific neighborhood\n",
    "        pol = (plane.loc[plane.larger_plane==h])\n",
    "        pol.reset_index(drop = True, inplace = True)\n",
    "        # Identify those records from points that are intersecting with the region polygon \n",
    "        #  1st (point) is more precise but does not work with all records\n",
    "        #  or a polygon intersecting with a polygon. We risk duplicates on 2nd option. \n",
    "        if points.geom_type[0] == 'Point':\n",
    "            pip_mask = points.within(pol.loc[0, 'geometry'])\n",
    "        elif (points.geom_type[0] == 'Polygon' or points.geom_type[0] == 'MultiPolygon'):\n",
    "            pip_mask = points.intersects(pol.loc[0, 'geometry'])  \n",
    "        # Filter points to keep only the intersecting records\n",
    "        pip_data = points.loc[pip_mask].copy()\n",
    "        # Create a new column and assign the region name as the value\n",
    "        pip_data['assigned_plane']= h\n",
    "        # Append filling venue data to empty dataframe\n",
    "        df = df.append(pip_data)\n",
    "    #checking there are no more than one larger shape assigned to a smaller shape or point   \n",
    "    print('Original dataframe count=',len(points),'\\nNew dataframe count=', len(df))\n",
    "    if df.loc[df.subplane.duplicated() == True].shape[0] > 0:\n",
    "        print(\"There are subpolygons assigned to more than one polygon. Count=\", df.loc[df.subplane.duplicated() == True].shape[0])\n",
    "        if df.loc[df.subplane.duplicated() == True].shape[0] < 6:\n",
    "            print(df.loc[df.subplane.duplicated() == True]['subplane'])\n",
    "    else:\n",
    "        print(\"No duplicates!\")    \n",
    "    # Checking all of our smaller shapes or points have not been assigned to a larger shape\n",
    "    if points.loc[~points.subplane.isin(df.subplane)].shape[0] > 0:\n",
    "        print(\"There are subpolygons without an assigned polygon. Count=\", points.loc[~points.subplane.isin(df.subplane)].shape[0])\n",
    "        if points.loc[~points.subplane.isin(df.subplane)].shape[0] < 6:\n",
    "            print(points.loc[~points.subplane.isin(df.subplane)]['subplane'])\n",
    "    else:\n",
    "        print(\"No unassigned!\")\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    #df = df.drop(columns='geometry')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the files in to pandas then a geopanda version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "isw_rfp_updated = pd.read_csv(isw_rfp_updated_csv)\n",
    "isw_rfp_updated = isw_rfp_updated.drop_duplicates()\n",
    "isw_rfp_updated = isw_rfp_updated.reset_index(drop=True)\n",
    "isw_rfp_updated['id'] = isw_rfp_updated.index\n",
    "isw_rfp_updated_gpd = gpd.GeoDataFrame(isw_rfp_updated, \n",
    "                                       geometry=gpd.points_from_xy(isw_rfp_updated.lng,\n",
    "                                                                   isw_rfp_updated.lat))\n",
    "isw_rfp_updated_gpd = isw_rfp_updated_gpd.rename(columns={'id':'subplane'})\n",
    "isw_rfp_updated_gpd['assigned_plane'] = ''\n",
    "\n",
    "cook_tract_geojson=gpd.read_file(cook_tract_geojson_file)\n",
    "cook_tract_geojson=cook_tract_geojson.rename(columns={'CensusTract':'larger_plane'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign a tract number to RFP locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working with Point.\n",
      "Original dataframe count= 39 \n",
      "New dataframe count= 39\n",
      "No duplicates!\n",
      "No unassigned!\n"
     ]
    }
   ],
   "source": [
    "isw_rfp_tract_gpd = get_pip(isw_rfp_updated_gpd, cook_tract_geojson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data frame as file, a GeoJSON and a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "isw_rfp_tract_gpd.drop('subplane', axis=1, inplace=True)\n",
    "isw_rfp_tract_gpd = isw_rfp_tract_gpd.rename(columns={'assigned_plane': 'CensusTract'})\n",
    "isw_rfp_tract_gpd.to_file(isw_rfp_tract_geojson, driver='GeoJSON')\n",
    "isw_rfp_tract_pd = pd.DataFrame(isw_rfp_tract_gpd.loc[:, isw_rfp_tract_gpd.columns != 'geometry'])\n",
    "isw_rfp_tract_pd.to_csv(isw_rfp_tract_csv, index=False)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the files from the file system to the IBM cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBM cloud only cell. Store the file to cloud storage so we can download to other environments\n",
    "# The file will be in the bucket but not appear as project asset until added on the console.\n",
    "\n",
    "key_isw_rfp_tract_geojson=os.path.basename(isw_rfp_tract_geojson)\n",
    "cos.upload_file(Filename=isw_rfp_tract_geojson, Bucket=credentials['BUCKET'],Key=key_isw_rfp_tract_geojson)\n",
    "\n",
    "key_isw_rfp_tract_csv=os.path.basename(isw_rfp_tract_csv)\n",
    "cos.upload_file(Filename=isw_rfp_tract_csv, Bucket=credentials['BUCKET'],Key=key_isw_rfp_tract_csv)\n",
    "\n",
    "key_cook_tract_geojson_file=os.path.basename(cook_tract_geojson_file)\n",
    "cos.upload_file(Filename=cook_tract_geojson_file, Bucket=credentials['BUCKET'],Key=key_cook_tract_geojson_file)\n",
    "\n",
    "key_cook_tract_csv_file=os.path.basename(cook_tract_csv_file)\n",
    "cos.upload_file(Filename=cook_tract_csv_file, Bucket=credentials['BUCKET'],Key=key_cook_tract_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
